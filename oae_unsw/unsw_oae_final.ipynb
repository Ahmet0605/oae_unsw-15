{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T00:22:22.855513Z",
     "start_time": "2024-12-05T00:22:22.301269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/AHMET/PycharmProjects/oae_unsw\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/AHMET/PycharmProjects/oae_unsw\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ydata_profiling\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score,classification_report,roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.autoencoder import autoencoder\n",
    "from utils.Data_gen import Data_gen\n",
    "from utils.get_result import get_result\n",
    "\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "train_data_df = pd.read_csv(\"./data/unsw/UNSW_NB15_training-set.csv\")\n",
    "test_data_df = pd.read_csv(\"./data/unsw/UNSW_NB15_testing-set.csv\")\n",
    "\n",
    "##label 0:n, 1:an\n",
    "data_df=pd.concat([train_data_df,test_data_df],axis=0)\n",
    "data_df=data_df.drop([\"id\"],axis=1)\n",
    "data_df=data_df.drop([\"attack_cat\"],axis=1)\n",
    "\n",
    "cat_vars=[\"proto\",\"service\",\"state\",\"is_ftp_login\",\"is_sm_ips_ports\"]\n",
    "cat_data = pd.get_dummies(data_df[cat_vars])\n",
    "\n",
    "numeric_vars = list(set(data_df.columns.values.tolist()) - set(cat_vars))\n",
    "numeric_vars.remove('label')\n",
    "numeric_data = data_df[numeric_vars].copy()\n",
    "\n",
    "label_data = data_df['label']\n",
    "\n",
    "final_data_df=pd.concat([numeric_data, cat_data, label_data], axis=1).reset_index(drop=True)"
   ],
   "id": "ea737e3d33131dee",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T00:23:14.060898Z",
     "start_time": "2024-12-05T00:23:13.463408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = final_data_df['label'].copy()\n",
    "num_data = len(labels)\n",
    "num_normal = np.sum(labels ==0)\n",
    "normal_ratio = num_normal/num_data\n",
    "print('normal_ratio(%)', normal_ratio*100, num_normal)\n",
    "\n",
    "def reduce_data(df, ano_cnt):\n",
    "    #num_anomalies = int(num_normal * (anomal_ratio /(1-anomal_ratio)))\n",
    "    num_anomalies =ano_cnt\n",
    "    print('number of anomalies = ', num_anomalies, 'number of normal = ', num_normal)\n",
    "    anomal_labels = labels[labels !=0]\n",
    "    anomal_idx = np.random.choice(anomal_labels.index, size = num_anomalies, replace = False)\n",
    "    anomal_data = df.iloc[anomal_idx].copy()\n",
    "    normal_data = df[labels ==0].copy()\n",
    "    print('anomal_data shape = ', anomal_data.shape)\n",
    "    print('normal_data shape = ',normal_data.shape)\n",
    "    return pd.concat([normal_data, anomal_data], axis = 0).reset_index(drop=True)\n",
    "\n",
    "ano_cnt=5000\n",
    "final_data_reduce_df = reduce_data(final_data_df, ano_cnt)\n",
    "print('reduced_data shape = ', final_data_reduce_df.shape)\n",
    "\n",
    "input_size=len(final_data_reduce_df.columns)-1\n",
    "\n",
    "\n",
    "test_an_df=final_data_reduce_df.loc[final_data_reduce_df[\"label\"]==1].reset_index(drop=True)\n",
    "n_df=final_data_reduce_df.loc[final_data_reduce_df[\"label\"]==0].reset_index(drop=True)\n",
    "n_df=sklearn.utils.shuffle(n_df).reset_index(drop=True)\n",
    "test_n_df=n_df.iloc[-ano_cnt:].reset_index(drop=True)\n",
    "test_df=pd.concat([test_an_df,test_n_df],axis=0).reset_index(drop=True)\n",
    "train_df=n_df.iloc[:-ano_cnt].reset_index(drop=True)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_df = min_max_scaler.fit_transform(train_df)\n",
    "test_df = min_max_scaler.transform(test_df)\n",
    "\n",
    "train_df = pd.DataFrame(train_df)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_label=train_df[192]\n",
    "train_df=train_df.drop([192],axis=1)\n",
    "\n",
    "y_test=test_df[192]\n",
    "x_test=test_df.drop([192],axis=1)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df,\n",
    "                                                    train_label,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1029)"
   ],
   "id": "50c35a195d19d877",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_ratio(%) 67.47558251941732 20242\n",
      "number of anomalies =  5000 number of normal =  20242\n",
      "anomal_data shape =  (5000, 193)\n",
      "normal_data shape =  (20242, 193)\n",
      "reduced_data shape =  (25242, 193)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T00:06:40.041946Z",
     "start_time": "2024-12-05T00:05:21.729029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.Data_gen import Data_gen  # utils klasöründeki Data_gen sınıfını içe aktarıyoruz\n",
    "from utils.autoencoder import autoencoder  # utils klasöründen autoencoder\n",
    "import numpy as np\n",
    "\n",
    "# Dataset'leri oluştur\n",
    "x_train_dataset = Data_gen(x_train)\n",
    "x_val_dataset = Data_gen(x_val)\n",
    "x_test_dataset = Data_gen(x_test)\n",
    "\n",
    "# Dataloader'ları tanımla\n",
    "batch_size = 64  # Batch boyutunu belirleyin\n",
    "train_dataloader = DataLoader(x_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(x_val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(x_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Modeli eğitme kısmı\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Kullanılan cihaz: {device}\")\n",
    "\n",
    "model = autoencoder(input_size).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "save_model_path = 'ae_model.pth'\n",
    "FULL_TRAIN = True\n",
    "num_epochs = 100\n",
    "\n",
    "if FULL_TRAIN:\n",
    "    for epoch in range(num_epochs):\n",
    "        running_training_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        # Eğitim döngüsü\n",
    "        for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "            sample_batched = sample_batched.to(device)\n",
    "            en_out, de_out, latant_v, output = model(sample_batched)\n",
    "            train_loss = criterion(output, sample_batched)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_training_loss += train_loss.item()\n",
    "\n",
    "        # Doğrulama döngüsü\n",
    "        if epoch % 1 == 0:\n",
    "            for i_batch, sample_batched in enumerate(val_dataloader):\n",
    "                sample_batched = sample_batched.to(device)\n",
    "                en_out, de_out, latant_v, val_output = model(sample_batched)\n",
    "                val_loss = criterion(val_output, sample_batched)\n",
    "                running_val_loss += val_loss.item()\n",
    "            print(\">>>> @ {}, training_loss = {:.4f}, val_loss = {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                running_training_loss / len(train_dataloader),\n",
    "                running_val_loss / len(val_dataloader)\n",
    "            ))\n",
    "\n",
    "        # Modeli kaydet\n",
    "        torch.save(model.state_dict(), save_model_path)\n",
    "else:\n",
    "    print(\"Önceden eğitilmiş modeli yüklüyor\")\n",
    "    model.load_state_dict(torch.load(save_model_path))\n",
    "\n",
    "# Test ve Eğitim Verilerinde Çalıştırma\n",
    "# Test verilerini işleyin\n",
    "x_test_array = x_test.values\n",
    "x_test_torch = torch.from_numpy(x_test_array).type(torch.FloatTensor).to(device)\n",
    "test_en_out, test_de_out, test_latant_v, x_test_recon = model(x_test_torch)\n",
    "\n",
    "# Eğitim verilerini işleyin\n",
    "x_train_array = x_train.values\n",
    "x_train_torch = torch.from_numpy(x_train_array).type(torch.FloatTensor).to(device)\n",
    "train_en_out, train_de_out, train_latant_v, x_train_recon = model(x_train_torch)\n"
   ],
   "id": "a15d67bf8fa3322a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan cihaz: cpu\n",
      ">>>> @ 1, training_loss = 0.0051, val_loss = 0.0033\n",
      ">>>> @ 2, training_loss = 0.0028, val_loss = 0.0023\n",
      ">>>> @ 3, training_loss = 0.0021, val_loss = 0.0018\n",
      ">>>> @ 4, training_loss = 0.0016, val_loss = 0.0013\n",
      ">>>> @ 5, training_loss = 0.0013, val_loss = 0.0011\n",
      ">>>> @ 6, training_loss = 0.0009, val_loss = 0.0009\n",
      ">>>> @ 7, training_loss = 0.0033, val_loss = 0.0039\n",
      ">>>> @ 8, training_loss = 0.0018, val_loss = 0.0021\n",
      ">>>> @ 9, training_loss = 0.0012, val_loss = 0.0011\n",
      ">>>> @ 10, training_loss = 0.0010, val_loss = 0.0009\n",
      ">>>> @ 11, training_loss = 0.0009, val_loss = 0.0009\n",
      ">>>> @ 12, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 13, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 14, training_loss = 0.0008, val_loss = 0.0007\n",
      ">>>> @ 15, training_loss = 0.0019, val_loss = 0.0017\n",
      ">>>> @ 16, training_loss = 0.0014, val_loss = 0.0013\n",
      ">>>> @ 17, training_loss = 0.0011, val_loss = 0.0009\n",
      ">>>> @ 18, training_loss = 0.0009, val_loss = 0.0010\n",
      ">>>> @ 19, training_loss = 0.0009, val_loss = 0.0009\n",
      ">>>> @ 20, training_loss = 0.0009, val_loss = 0.0009\n",
      ">>>> @ 21, training_loss = 0.0010, val_loss = 0.0010\n",
      ">>>> @ 22, training_loss = 0.0009, val_loss = 0.0010\n",
      ">>>> @ 23, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 24, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 25, training_loss = 0.0007, val_loss = 0.0007\n",
      ">>>> @ 26, training_loss = 0.0008, val_loss = 0.0007\n",
      ">>>> @ 27, training_loss = 0.0007, val_loss = 0.0007\n",
      ">>>> @ 28, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 29, training_loss = 0.0009, val_loss = 0.0007\n",
      ">>>> @ 30, training_loss = 0.0007, val_loss = 0.0007\n",
      ">>>> @ 31, training_loss = 0.0010, val_loss = 0.0022\n",
      ">>>> @ 32, training_loss = 0.0017, val_loss = 0.0022\n",
      ">>>> @ 33, training_loss = 0.0015, val_loss = 0.0012\n",
      ">>>> @ 34, training_loss = 0.0013, val_loss = 0.0011\n",
      ">>>> @ 35, training_loss = 0.0011, val_loss = 0.0010\n",
      ">>>> @ 36, training_loss = 0.0010, val_loss = 0.0010\n",
      ">>>> @ 37, training_loss = 0.0010, val_loss = 0.0008\n",
      ">>>> @ 38, training_loss = 0.0008, val_loss = 0.0008\n",
      ">>>> @ 39, training_loss = 0.0008, val_loss = 0.0007\n",
      ">>>> @ 40, training_loss = 0.0039, val_loss = 0.0060\n",
      ">>>> @ 41, training_loss = 0.0045, val_loss = 0.0040\n",
      ">>>> @ 42, training_loss = 0.0037, val_loss = 0.0034\n",
      ">>>> @ 43, training_loss = 0.0034, val_loss = 0.0034\n",
      ">>>> @ 44, training_loss = 0.0033, val_loss = 0.0038\n",
      ">>>> @ 45, training_loss = 0.0029, val_loss = 0.0025\n",
      ">>>> @ 46, training_loss = 0.0024, val_loss = 0.0023\n",
      ">>>> @ 47, training_loss = 0.0024, val_loss = 0.0023\n",
      ">>>> @ 48, training_loss = 0.0023, val_loss = 0.0024\n",
      ">>>> @ 49, training_loss = 0.0027, val_loss = 0.0028\n",
      ">>>> @ 50, training_loss = 0.0026, val_loss = 0.0025\n",
      ">>>> @ 51, training_loss = 0.0025, val_loss = 0.0025\n",
      ">>>> @ 52, training_loss = 0.0023, val_loss = 0.0026\n",
      ">>>> @ 53, training_loss = 0.0033, val_loss = 0.0025\n",
      ">>>> @ 54, training_loss = 0.0219, val_loss = 0.0042\n",
      ">>>> @ 55, training_loss = 0.0038, val_loss = 0.0037\n",
      ">>>> @ 56, training_loss = 0.0035, val_loss = 0.0035\n",
      ">>>> @ 57, training_loss = 0.0040, val_loss = 0.0038\n",
      ">>>> @ 58, training_loss = 0.0036, val_loss = 0.0037\n",
      ">>>> @ 59, training_loss = 0.0035, val_loss = 0.0035\n",
      ">>>> @ 60, training_loss = 0.0038, val_loss = 0.0047\n",
      ">>>> @ 61, training_loss = 0.0038, val_loss = 0.0033\n",
      ">>>> @ 62, training_loss = 0.0031, val_loss = 0.0031\n",
      ">>>> @ 63, training_loss = 5001.1789, val_loss = 0.1899\n",
      ">>>> @ 64, training_loss = 0.1232, val_loss = 0.1184\n",
      ">>>> @ 65, training_loss = 0.0748, val_loss = 0.0684\n",
      ">>>> @ 66, training_loss = 0.0565, val_loss = 0.0560\n",
      ">>>> @ 67, training_loss = 0.0460, val_loss = 0.0487\n",
      ">>>> @ 68, training_loss = 0.0394, val_loss = 0.0447\n",
      ">>>> @ 69, training_loss = 0.0348, val_loss = 0.0378\n",
      ">>>> @ 70, training_loss = 0.0318, val_loss = 0.0325\n",
      ">>>> @ 71, training_loss = 0.0289, val_loss = 0.0292\n",
      ">>>> @ 72, training_loss = 0.0269, val_loss = 0.0269\n",
      ">>>> @ 73, training_loss = 0.0254, val_loss = 0.0254\n",
      ">>>> @ 74, training_loss = 0.0229, val_loss = 0.0228\n",
      ">>>> @ 75, training_loss = 0.0219, val_loss = 0.0219\n",
      ">>>> @ 76, training_loss = 0.0210, val_loss = 0.0212\n",
      ">>>> @ 77, training_loss = 0.0204, val_loss = 0.0206\n",
      ">>>> @ 78, training_loss = 0.0198, val_loss = 0.0201\n",
      ">>>> @ 79, training_loss = 0.0193, val_loss = 0.0194\n",
      ">>>> @ 80, training_loss = 0.0185, val_loss = 0.0184\n",
      ">>>> @ 81, training_loss = 0.0174, val_loss = 0.0174\n",
      ">>>> @ 82, training_loss = 0.0167, val_loss = 0.0168\n",
      ">>>> @ 83, training_loss = 0.0160, val_loss = 0.0163\n",
      ">>>> @ 84, training_loss = 0.0157, val_loss = 0.0160\n",
      ">>>> @ 85, training_loss = 0.0153, val_loss = 0.0157\n",
      ">>>> @ 86, training_loss = 0.0150, val_loss = 0.0154\n",
      ">>>> @ 87, training_loss = 0.0148, val_loss = 0.0152\n",
      ">>>> @ 88, training_loss = 0.0146, val_loss = 0.0150\n",
      ">>>> @ 89, training_loss = 0.0145, val_loss = 0.0147\n",
      ">>>> @ 90, training_loss = 0.0142, val_loss = 0.0146\n",
      ">>>> @ 91, training_loss = 0.0141, val_loss = 0.0144\n",
      ">>>> @ 92, training_loss = 0.0140, val_loss = 0.0142\n",
      ">>>> @ 93, training_loss = 0.0138, val_loss = 0.0141\n",
      ">>>> @ 94, training_loss = 0.0137, val_loss = 0.0139\n",
      ">>>> @ 95, training_loss = 0.0135, val_loss = 0.0137\n",
      ">>>> @ 96, training_loss = 0.0133, val_loss = 0.0135\n",
      ">>>> @ 97, training_loss = 0.0132, val_loss = 0.0133\n",
      ">>>> @ 98, training_loss = 0.0130, val_loss = 0.0132\n",
      ">>>> @ 99, training_loss = 0.0128, val_loss = 0.0130\n",
      ">>>> @ 100, training_loss = 0.0126, val_loss = 0.0127\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T00:07:58.248665Z",
     "start_time": "2024-12-05T00:07:55.076939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import covariance\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "# Empirical Covariance\n",
    "group_lasso = covariance.EmpiricalCovariance(assume_centered=False)\n",
    "group_lasso.fit(x_train_array)\n",
    "incov = group_lasso.precision_\n",
    "mean = np.mean(x_train_array, axis=0)\n",
    "mean = list(mean)\n",
    "\n",
    "scores = []\n",
    "for i in range(len(x_test_array)):\n",
    "    scores.append(distance.mahalanobis(mean, x_test_array[i], incov))\n",
    "ma_scores = np.array(scores)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_Forest = IsolationForest(n_estimators=100, random_state=123)\n",
    "iso_Forest.fit(x_train_array)\n",
    "if_scores = iso_Forest.decision_function(x_test_array)\n",
    "\n",
    "# Autoencoder Scores (AE Scores)\n",
    "x_test_recon_np = x_test_recon.detach().cpu().numpy()  # Tensörü NumPy'e çevirme\n",
    "ae_scores = np.mean((x_test_array - x_test_recon_np) ** 2, axis=1)\n",
    "\n",
    "# Diğer inputlar için işlemler\n",
    "inputs = [\n",
    "    [train_en_out[0].detach().cpu().numpy(), test_en_out[0].detach().cpu().numpy()],\n",
    "    [train_en_out[1].detach().cpu().numpy(), test_en_out[1].detach().cpu().numpy()],\n",
    "    [train_en_out[2].detach().cpu().numpy(), test_en_out[2].detach().cpu().numpy()],\n",
    "    [train_en_out[3].detach().cpu().numpy(), test_en_out[3].detach().cpu().numpy()],\n",
    "    [train_latant_v.detach().cpu().numpy(), test_latant_v.detach().cpu().numpy()],\n",
    "    [train_de_out[0].detach().cpu().numpy(), test_de_out[0].detach().cpu().numpy()],\n",
    "    [train_de_out[1].detach().cpu().numpy(), test_de_out[1].detach().cpu().numpy()],\n",
    "    [train_de_out[2].detach().cpu().numpy(), test_de_out[2].detach().cpu().numpy()],\n",
    "    [train_de_out[3].detach().cpu().numpy(), test_de_out[3].detach().cpu().numpy()],\n",
    "    [train_de_out[4].detach().cpu().numpy(), test_de_out[4].detach().cpu().numpy()],\n",
    "]\n",
    "\n",
    "efe_scores = []\n",
    "dfe_scores = []\n",
    "\n",
    "for inp in inputs[:5]:  # EFE Scores için\n",
    "    group_lasso.fit(inp[0])\n",
    "    incov = group_lasso.precision_\n",
    "    mean = np.mean(inp[0], axis=0)\n",
    "    mean = list(mean)\n",
    "    scores = []\n",
    "    for i in range(len(inp[1])):\n",
    "        scores.append(distance.mahalanobis(mean, inp[1][i], incov))\n",
    "    efe_scores.append(np.array(scores))\n",
    "\n",
    "for inp in inputs[5:]:  # DFE Scores için\n",
    "    group_lasso.fit(inp[0])\n",
    "    incov = group_lasso.precision_\n",
    "    mean = np.mean(inp[0], axis=0)\n",
    "    mean = list(mean)\n",
    "    scores = []\n",
    "    for i in range(len(inp[1])):\n",
    "        scores.append(distance.mahalanobis(mean, inp[1][i], incov))\n",
    "    dfe_scores.append(np.array(scores))\n",
    "\n",
    "# Tüm skorları birleştir\n",
    "scores = [ma_scores] + [if_scores] + [ae_scores] + efe_scores + dfe_scores\n"
   ],
   "id": "5eda880ffeb04535",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:50:25.356571Z",
     "start_time": "2024-12-04T23:50:22.656598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "scores_df=pd.DataFrame({\"ma_scores\":scores[0],\n",
    "                        \"if_scores\":scores[1],\n",
    "                        \"ae_scores\":scores[2],\n",
    "                        \"efe_scores_1\":scores[3],\n",
    "                        \"efe_scores_2\":scores[4],\n",
    "                        \"efe_scores_3\":scores[5],\n",
    "                        \"efe_scores_4\":scores[6],\n",
    "                        \"efe_scores_5\":scores[7],\n",
    "                        \"dfe_scores_1\":scores[8],\n",
    "                        \"dfe_scores_2\":scores[9],\n",
    "                        \"dfe_scores_3\":scores[10],\n",
    "                        \"dfe_scores_4\":scores[11],\n",
    "                        \"dfe_scores_5\":scores[12],\n",
    "                        \"label\":y_test})\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ma_scores\") #only mahal of input\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"if_scores\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"if_scores\") #isolation forest\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ae_scores\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ae_scores\") # use recon loss\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"efe_scores\") # use encoder mahal \n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"dfe_scores\") # use decoder mahal\n",
    "print(test_acc) \n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"mahal+efe_scores\") # use mahal for input and encoder mahal\n",
    "print(test_acc) \n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"mahal+dfe_scores\") # use mahal for input and decoder mahal\n",
    "print(test_acc) \n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"ae_scores\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"mahal+ae_scores\") # use mahal for input and recon loss\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"efe_scores+dfe_scores\") # use encoder mahal and decoder mahal\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ae_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ae_scores+efe_scores\") # use encoder mahal and recon loss\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ae_scores\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ae_scores+dfe_scores\") # use decoder mahal and recon loss\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ma_scores+efe_scores+dfe_scores\") # # use mahal for input, encoder mahal and decoder mahal\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"ae_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ma_scores+ae_scores+efe_scores\") # # use mahal for input, recon loss, encoder mahal\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"ae_scores\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ma_scores+ae_scores+dfe_scores\") # # use mahal for input, recon loss, decoder mahal\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ae_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ae_scores+efe_scores+dfe_scores\") # # use recon loss, encoder mahal, and decoder mahal\n",
    "print(test_acc)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")\n",
    "\n",
    "x=scores_df.drop(\"label\",axis=1)\n",
    "x=scores_df[[\"ma_scores\",\"ae_scores\",\"efe_scores_1\",\"efe_scores_2\",\"efe_scores_3\",\"efe_scores_4\",\"efe_scores_5\",\"dfe_scores_1\",\"dfe_scores_2\",\"dfe_scores_3\",\"dfe_scores_4\",\"dfe_scores_5\"]]\n",
    "y=scores_df[\"label\"].values\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "model=clf\n",
    "test_acc = model.score(x, y)\n",
    "AUROC = roc_auc_score(y,model.predict_proba(x)[:,1])\n",
    "AUPRC = average_precision_score(y,model.predict_proba(x)[:,1])\n",
    "recall = recall_score(y,  model.predict(x))\n",
    "precision = precision_score(y, model.predict(x))\n",
    "f1_score_=f1_score(y_true=y, y_pred =model.predict(x))\n",
    "fpr, tpr, thresholds = roc_curve(y,model.predict_proba(x)[:,1])\n",
    "tn, fp, fn, tp = confusion_matrix(scores_df[\"label\"].values, model.predict(x)).ravel()\n",
    "\n",
    "print(\"ma_scores+ae_scores+efe_scores+dfe_scores\") # # use mahal for input, recon loss, encoder mahal, and decoder mahal\n",
    "print(test_acc)\n",
    "print(AUROC)\n",
    "print(AUPRC)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score_)\n",
    "print(fp/(fp+tn))\n",
    "print(\"\\n\")"
   ],
   "id": "db6694c82c161aa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma_scores\n",
      "0.8999\n",
      "0.9347684279191129\n",
      "0.8598\n",
      "0.9806419599999999\n",
      "0.9727128447978912\n",
      "0.895718303989999\n",
      "0.06\n",
      "\n",
      "\n",
      "if_scores\n",
      "0.9345\n",
      "0.9214354995150339\n",
      "0.95\n",
      "0.9597928\n",
      "0.9065252018995851\n",
      "0.9354997538158543\n",
      "0.081\n",
      "\n",
      "\n",
      "ae_scores\n",
      "0.8155\n",
      "0.9349324510614834\n",
      "0.6782\n",
      "0.9707798000000001\n",
      "0.963063082187282\n",
      "0.7861365480468298\n",
      "0.0472\n",
      "\n",
      "\n",
      "efe_scores\n",
      "0.9274\n",
      "0.9508438818565401\n",
      "0.9014\n",
      "0.98504532\n",
      "0.9834315933085407\n",
      "0.9254620123203285\n",
      "0.0466\n",
      "\n",
      "\n",
      "dfe_scores\n",
      "0.8647\n",
      "0.9309855826045852\n",
      "0.7878\n",
      "0.92960526\n",
      "0.9176412064254541\n",
      "0.8534286642833929\n",
      "0.0584\n",
      "\n",
      "\n",
      "mahal+efe_scores\n",
      "0.9262\n",
      "0.94957805907173\n",
      "0.9002\n",
      "0.9853526800000002\n",
      "0.983724558747253\n",
      "0.9242299794661191\n",
      "0.0478\n",
      "\n",
      "\n",
      "mahal+dfe_scores\n",
      "0.8965\n",
      "0.938121546961326\n",
      "0.849\n",
      "0.9811413199999999\n",
      "0.9741051213393801\n",
      "0.8913385826771654\n",
      "0.056\n",
      "\n",
      "\n",
      "mahal+ae_scores\n",
      "0.9002\n",
      "0.935\n",
      "0.8602\n",
      "0.9806946799999998\n",
      "0.9727890360137151\n",
      "0.8960416666666666\n",
      "0.0598\n",
      "\n",
      "\n",
      "efe_scores+dfe_scores\n",
      "0.9285\n",
      "0.9530556142947769\n",
      "0.9014\n",
      "0.9853533200000001\n",
      "0.9842253604546489\n",
      "0.9265083770171652\n",
      "0.0444\n",
      "\n",
      "\n",
      "ae_scores+efe_scores\n",
      "0.9272\n",
      "0.950632911392405\n",
      "0.9012\n",
      "0.98507168\n",
      "0.9834527204016094\n",
      "0.9252566735112936\n",
      "0.0468\n",
      "\n",
      "\n",
      "ae_scores+dfe_scores\n",
      "0.8669\n",
      "0.9354379302159981\n",
      "0.7882\n",
      "0.9404824799999998\n",
      "0.9253610382695643\n",
      "0.855530229024205\n",
      "0.0544\n",
      "\n",
      "\n",
      "ma_scores+efe_scores+dfe_scores\n",
      "0.9258\n",
      "0.9522090059473237\n",
      "0.8966\n",
      "0.9855379200000002\n",
      "0.98460803994012\n",
      "0.9235681911825299\n",
      "0.045\n",
      "\n",
      "\n",
      "ma_scores+ae_scores+efe_scores\n",
      "0.9264\n",
      "0.9495993251792493\n",
      "0.9006\n",
      "0.98537144\n",
      "0.9837375532135054\n",
      "0.9244508314514474\n",
      "0.0478\n",
      "\n",
      "\n",
      "ma_scores+ae_scores+dfe_scores\n",
      "0.8964\n",
      "0.9381078691423519\n",
      "0.8488\n",
      "0.98114244\n",
      "0.9741227905756928\n",
      "0.8912221755564889\n",
      "0.056\n",
      "\n",
      "\n",
      "ae_scores+efe_scores+dfe_scores\n",
      "0.9281\n",
      "0.9528242013962344\n",
      "0.9008\n",
      "0.9854769600000001\n",
      "0.9842690701322477\n",
      "0.9260820396833556\n",
      "0.0446\n",
      "\n",
      "\n",
      "ma_scores+ae_scores+efe_scores+dfe_scores\n",
      "0.9266\n",
      "0.98524876\n",
      "0.9839523781963574\n",
      "0.949810206663855\n",
      "0.9008\n",
      "0.9246561281051119\n",
      "0.0476\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "991996cb42c1184e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
